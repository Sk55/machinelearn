{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        使用决策树二元分类分析StumbleUpon数据集，预测网页是暂时性（Ephemeral）或是长青的（Evergreen），\n",
    "    并且调校参数找出最佳参数组合，提高预测准确度。\n",
    "        决策树的优点：条例清晰、方法简单、易于理解、使用范围广等。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    决策树介绍：\n",
    "        当我们使用决策树分类算法训练数据后，会以 feature(特征字段)与label(标签字段)建立决策树。如下图所示：\n",
    "    使用 湿度 与 气压（feature 特征字段）来判断天气为“晴”或“雨”（label 标签字段，也就是预测的目标）。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./datas/decisionTree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        当我们使用历史数据执行训练时会建立决策树。可是决策树不可能无限成长，因此我们必须限制它的最大分支与深度，\n",
    "    所以必须设置下列参数：\n",
    "        -a. maxBins 参数：\n",
    "            决策时每一个节点最大分支数\n",
    "        -b. maxDepth 参数：\n",
    "            决策树最大深度\n",
    "        -c. Impurity 参数：\n",
    "            决策树分裂节点时的方法，为什么选择特征进行分支\n",
    "        当树的父节点在分裂子节点时，以什么方法作为依据？例如，湿度以60为分割点，分为大于60或小于60；或者湿度\n",
    "    以50为分割点，分为大于50或小于50.到底哪种方式比较好呢？此时有Gini 与 Entropy 两种判断方式：\n",
    "        -i. 基于系数（Gini）：\n",
    "            由意大利统计学家Corrado Gini 发明，用于计算数值散步程度（Statistical Dispersion，统计离差）的指标。\n",
    "        决策树算法对每种特征字段分割点计算估值，选择分裂后最小的基尼指数（Gini）方式。\n",
    "        -ii. 熵（Entropy）:\n",
    "            熵（Entropy）也被用于计算机系统混乱的程度。决策树算法对每种特征字段分割点计算估值，选择分裂后最小\n",
    "        的熵（Entropy）方式。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle数据分析竞赛平台"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Kaggle 网站介绍\n",
    "        -a. 数据分析竞赛平台，企业或研究者将大数据的问题发布到网站上，包括数据、问题说明、期望的目标、奖金等，以\n",
    "            向大众征求解决方案。\n",
    "                http://www.kaggle.com/\n",
    "        -b. 网络上任何人都可以参与大数据问题的竞赛：\n",
    "                下载数据、分析数据、运行机器学习、数据挖掘等知识，建立算法模型并解决问题，最后将结果上传到网站上。\n",
    "            如果提交申请的结果符合要求，并且在参赛者种排名第一，就可以获得奖金。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StumbleUpon Evergreen 竞赛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    1、Kaggle 网站上一个竞赛：StumbleUpon Evergreen Classification Challenge\n",
    "        https://www.kaggle.com/c/stumbleupon\n",
    "    2、StumbleUpon是个性化的搜索引擎，会按用户的兴趣和网页评分等记录推荐给你感兴趣的网页，例如新文章、季节菜单、\n",
    "    新闻、教学等。超过数千万人使用StumbleUpon查找新网页、图片、影片.....\n",
    "    3、业务说明：\n",
    "        -a. 有些网页内容是暂时性的（ephemeral），例如季节菜单、当日股市涨跌新闻等。\n",
    "            这些文章可能只是某一段时间会对读者有意义，过了这段时间对读者就没有意义了。\n",
    "        -b. 有些网页内容是长青的（evergreen），例如理财观念、育儿知识等。\n",
    "            读者会长久对这些文章感兴趣。\n",
    "        -c. 分辨网页是暂时性（ephemeral）还是长青的（evergreen），对于StumbleUpon推荐网页给用户会有很大帮助。\n",
    "            例如 A 买卖股票，Ta可能对当日股市涨跌新闻感兴趣，可是过了一周就对这则新闻就没有兴趣了；如果是理财\n",
    "            观念的文章，读者A可能会对会长久感兴趣。\n",
    "        -d. 因此公司找来大数据分析师，负责“网页分类”大数据项目。\n",
    "            网页内容我们人类看过了，就可以大致分为暂时性的或是长青的，这就是历史数据。\n",
    "        -e. 目标就是利用机器学习（Machine Learning），通过大量网页数据进行训练来建立一个模型，并使用这个模型去\n",
    "            预测网页是属于 暂时性的 还是 长青的内容，此属于二元分类问题。\n",
    "        -f. 分类常见算法：\n",
    "            - 决策树分类\n",
    "            - 逻辑回归分类\n",
    "            - 支持向量机分类\n",
    "            - 朴素贝叶斯分类\n",
    "        \n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    构建机器学习模型步骤；\n",
    "        1、如何搜集数据 ？\n",
    "            历史数据\n",
    "        2、如何进行数据准备 ？\n",
    "            提取特征字段和标签字段  -   特征工程    （花费时间最多的）\n",
    "        3、如何训练模型 ？\n",
    "            使用什么算法进行训练模型\n",
    "        4、如何使用模型预测 ？\n",
    "            使用训练的模型如决策树模型，进行预测\n",
    "        5、如何评估模型的准确率?\n",
    "            使用某一个标准来评估模型的准确率，二元分类中使用 AUC 作为评估标准\n",
    "        6、模型训练参数如何影响准确率？\n",
    "            训练模型时，针对算法传递不同的参数将会影响准确率和训练时间。\n",
    "            如使用决策树算法，其中参数impurity、maxDepth、maxBins的值设置\n",
    "        7、如何找出准确率最高的参数组合？\n",
    "            不同的参数，不同的组合得到的模型不一样，准确率也不痒。\n",
    "        8、如何确认是否Overfiiting（过度训练，过拟合）：\n",
    "            Overfiting（过度训练）是指机器学习所学到的模型过度贴近trainData，从而导致误差变得很大。\n",
    "            我们使用另一组数据testData再次测试，以避免overfitting的问题。\n",
    "            - 如果训练评估阶段是AUC很高，但是测试阶段AUC很低，代表可能有overfitting的问题。\n",
    "            - 如果测试与训练评估阶段的结果中AUC差异不大，就代表无overfitting的问题。\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入模块 pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# 导入系统模块\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 设置环境变量\n",
    "os.environ['JAVA_HOME'] = 'C:\\Java\\jdk1.8.0_91'\n",
    "# HADOOP在Windows的兼容性问题  主要需要$HADOOP_HOME/lib中winutils.exe等文件\n",
    "os.environ['HADOOP_HOME'] = 'C:\\Java\\hadoop-2.6.0-cdh5.7.6'\n",
    "# 设置SPARK_HOME环境变量, 非常重要, 如果没有设置的话,SparkApplication运行不了\n",
    "os.environ['SPARK_HOME'] = 'C:\\Java\\spark-2.2.0-bin-2.6.0-cdh5.7.6'\n",
    "\n",
    "# Create SparkConf\n",
    "sparkConf = SparkConf()\\\n",
    "    .setAppName('Python_Spark_Classifier')\\\n",
    "    .setMaster('local[2]')\n",
    "# Create SparkContext\n",
    "sc = SparkContext(conf=sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[2] appName=Python_Spark_Classifier>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### StumbleUpon 数据集\n",
    "    https://www.kaggle.com/c/stumbleupon/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"url\"\\t\"urlid\"\\t\"boilerplate\"\\t\"alchemy_category\"\\t\"alchemy_category_score\"\\t\"avglinksize\"\\t\"commonlinkratio_1\"\\t\"commonlinkratio_2\"\\t\"commonlinkratio_3\"\\t\"commonlinkratio_4\"\\t\"compression_ratio\"\\t\"embed_ratio\"\\t\"framebased\"\\t\"frameTagRatio\"\\t\"hasDomainLink\"\\t\"html_ratio\"\\t\"image_ratio\"\\t\"is_news\"\\t\"lengthyLinkDomain\"\\t\"linkwordscore\"\\t\"news_front_page\"\\t\"non_markup_alphanum_characters\"\\t\"numberOfLinks\"\\t\"numwords_in_url\"\\t\"parametrizedLinkRatio\"\\t\"spelling_errors_ratio\"\\t\"label\"']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据集： 预测 网页是短暂的还是长青的\n",
    "raw_rdd = sc.textFile('./datas/train.tsv')\n",
    "\n",
    "raw_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_rdd.take(1)[0].split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    数据集中字段：\n",
    "        -a. 每行数据的各个字段使用制表符隔开 \\t\n",
    "            文件的第一行为字段名称\n",
    "        -b. 字段 0 - 2 \n",
    "            表示的是 url网址、urlid网址ID、boilerplate连接的样本文字,此三个字段与判断网页是否长青性关系不大，忽略\n",
    "        -c. 字段 3 - 25 \n",
    "            总共23个字段属于特征字段值，基本上都是数值类型特征\n",
    "        -d. 字段26\n",
    "            属于 标签label，具有两个值\n",
    "            - 0: 代表长青性（evergreen）\n",
    "            - 1： 代表的是短暂性\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"url\"\\t\"urlid\"\\t\"boilerplate\"\\t\"alchemy_category\"\\t\"alchemy_category_score\"\\t\"avglinksize\"\\t\"commonlinkratio_1\"\\t\"commonlinkratio_2\"\\t\"commonlinkratio_3\"\\t\"commonlinkratio_4\"\\t\"compression_ratio\"\\t\"embed_ratio\"\\t\"framebased\"\\t\"frameTagRatio\"\\t\"hasDomainLink\"\\t\"html_ratio\"\\t\"image_ratio\"\\t\"is_news\"\\t\"lengthyLinkDomain\"\\t\"linkwordscore\"\\t\"news_front_page\"\\t\"non_markup_alphanum_characters\"\\t\"numberOfLinks\"\\t\"numwords_in_url\"\\t\"parametrizedLinkRatio\"\\t\"spelling_errors_ratio\"\\t\"label\"',\n",
       " '\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"\\t\"4042\"\\t\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ball The predictions are part of an annual tradition for the Armonk New York based company which surveys its 3 000 researchers to find five ideas expected to take root in the next five years IBM the world s largest provider of computer services looks to Silicon Valley for input gleaning many ideas from its Almaden research center in San Jose California Holographic conversations projected from mobile phones lead this year s list The predictions also include air breathing batteries computer programs that can tell when and where traffic jams will take place environmental information generated by sensors in cars and phones and cities powered by the heat thrown off by computer servers These are all stretch goals and that s good said Paul Saffo managing director of foresight at the investment advisory firm Discern in San Francisco In an era when pessimism is the new black a little dose of technological optimism is not a bad thing For IBM it s not just idle speculation The company is one of the few big corporations investing in long range research projects and it counts on innovation to fuel growth Saffo said Not all of its predictions pan out though IBM was overly optimistic about the spread of speech technology for instance When the ideas do lead to products they can have broad implications for society as well as IBM s bottom line he said Research Spending They have continued to do research when all the other grand research organizations are gone said Saffo who is also a consulting associate professor at Stanford University IBM invested 5 8 billion in research and development last year 6 1 percent of revenue While that s down from about 10 percent in the early 1990s the company spends a bigger share on research than its computing rivals Hewlett Packard Co the top maker of personal computers spent 2 4 percent last year At Almaden scientists work on projects that don t always fit in with IBM s computer business The lab s research includes efforts to develop an electric car battery that runs 500 miles on one charge a filtration system for desalination and a program that shows changes in geographic data IBM rose 9 cents to 146 04 at 11 02 a m in New York Stock Exchange composite trading The stock had gained 11 percent this year before today Citizen Science The list is meant to give a window into the company s innovation engine said Josephine Cheng a vice president at IBM s Almaden lab All this demonstrates a real culture of innovation at IBM and willingness to devote itself to solving some of the world s biggest problems she said Many of the predictions are based on projects that IBM has in the works One of this year s ideas that sensors in cars wallets and personal devices will give scientists better data about the environment is an expansion of the company s citizen science initiative Earlier this year IBM teamed up with the California State Water Resources Control Board and the City of San Jose Environmental Services to help gather information about waterways Researchers from Almaden created an application that lets smartphone users snap photos of streams and creeks and report back on conditions The hope is that these casual observations will help local and state officials who don t have the resources to do the work themselves Traffic Predictors IBM also sees data helping shorten commutes in the next five years Computer programs will use algorithms and real time traffic information to predict which roads will have backups and how to avoid getting stuck Batteries may last 10 times longer in 2015 than today IBM says Rather than using the current lithium ion technology new models could rely on energy dense metals that only need to interact with the air to recharge Some electronic devices might ditch batteries altogether and use something similar to kinetic wristwatches which only need to be shaken to generate a charge The final prediction involves recycling the heat generated by computers and data centers Almost half of the power used by data centers is currently spent keeping the computers cool IBM scientists say it would be better to harness that heat to warm houses and offices In IBM s first list of predictions compiled at the end of 2006 researchers said instantaneous speech translation would become the norm That hasn t happened yet While some programs can quickly translate electronic documents and instant messages and other apps can perform limited speech translation there s nothing widely available that acts like the universal translator in Star Trek Second Life The company also predicted that online immersive environments such as Second Life would become more widespread While immersive video games are as popular as ever Second Life s growth has slowed Internet users are flocking instead to the more 2 D environments of Facebook Inc and Twitter Inc Meanwhile a 2007 prediction that mobile phones will act as a wallet ticket broker concierge bank and shopping assistant is coming true thanks to the explosion of smartphone applications Consumers can pay bills through their banking apps buy movie tickets and get instant feedback on potential purchases all with a few taps on their phones The nice thing about the list is that it provokes thought Saffo said If everything came true they wouldn t be doing their job To contact the reporter on this story Ryan Flinn in San Francisco at rflinn bloomberg net To contact the editor responsible for this story Tom Giles at tgiles5 bloomberg net by 2015, your mobile phone will project a 3-d image of anyone who calls and your laptop will be powered by kinetic energy. at least that\\\\u2019s what international business machines corp. sees in its crystal ball.\"\",\"\"url\"\":\"\"bloomberg news 2010 12 23 ibm predicts holographic calls air breathing batteries by 2015 html\"\"}\"\\t\"business\"\\t\"0.789131\"\\t\"2.055555556\"\\t\"0.676470588\"\\t\"0.205882353\"\\t\"0.047058824\"\\t\"0.023529412\"\\t\"0.443783175\"\\t\"0\"\\t\"0\"\\t\"0.09077381\"\\t\"0\"\\t\"0.245831182\"\\t\"0.003883495\"\\t\"1\"\\t\"1\"\\t\"24\"\\t\"0\"\\t\"5424\"\\t\"170\"\\t\"8\"\\t\"0.152941176\"\\t\"0.079129575\"\\t\"0\"']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取原始数据集中前2条数据\n",
    "raw_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取第一条数据\n",
    "header_data = raw_rdd.first()\n",
    "\n",
    "# 采用过滤的方式，删除第一条数据\n",
    "filter_rdd = raw_rdd.filter(lambda line: line != header_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'总共：7395'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替换数据中 引号，对每行数据按照制表符分割\n",
    "datas = filter_rdd \\\n",
    "    .map(lambda line: line.replace(\"\\\"\", \"\"))\\\n",
    "    .map(lambda line: line.split(\"\\t\"))\n",
    "    \n",
    "\"总共：\" + str(datas.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    数据分为三个部分：\\n        - 第一个部分：\\n            第一个字段：特征地址为 类别特征数据\\n        - 第二个部分：\\n            最后一个字段：label 字段\\n        - 第三个部分：\\n            其他字段：数值特征字段\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看第一条数据\n",
    "datas.first()[3:]\n",
    "\n",
    "\"\"\"\n",
    "    数据分为三个部分：\n",
    "        - 第一个部分：\n",
    "            第一个字段：特征地址为 类别特征数据\n",
    "        - 第二个部分：\n",
    "            最后一个字段：label 字段\n",
    "        - 第三个部分：\n",
    "            其他字段：数值特征字段\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 提取特征feature字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  编写函数extract_feature实现特征字段的提取：\n",
    "  1. 提取 类别特征 字段\n",
    "  2. 提取 数字特征 字段\n",
    "  3. 合并 类别特征 + 数值特征\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'?': 2,\n",
       " 'arts_entertainment': 3,\n",
       " 'business': 0,\n",
       " 'computer_internet': 6,\n",
       " 'culture_politics': 5,\n",
       " 'gaming': 4,\n",
       " 'health': 12,\n",
       " 'law_crime': 7,\n",
       " 'recreation': 11,\n",
       " 'religion': 8,\n",
       " 'science_technology': 13,\n",
       " 'sports': 1,\n",
       " 'unknown': 10,\n",
       " 'weather': 9}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类别特征数据转换：采用 1-of-K，其中K表示的就是类别的个数\n",
    "# 构建 网页类别 字典\n",
    "catetory_dic = datas \\\n",
    "    .map(lambda fields: fields[3]) \\\n",
    "    .distinct() \\\n",
    "    .zipWithIndex() \\\n",
    "    .collectAsMap()\n",
    "\n",
    "catetory_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看字典的 项数\n",
    "len(catetory_dic)\n",
    "\n",
    "# 查看类型\n",
    "type(catetory_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Numpy模块\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 转换 字段的值为 ? 转换为 0，转换为数值类型\n",
    "def conver_float(x):\n",
    "    return (0 if x == \"?\" else float(x))\n",
    "\n",
    "\n",
    "# 特征字段提取\n",
    "def extract_features(fields, catetory_dic, end_index):\n",
    "    # 类别字段\n",
    "    category_index = catetory_dic[fields[3]]\n",
    "    category_features = np.zeros(len(catetory_dic))\n",
    "    category_features[category_index] = 1.0\n",
    "    \n",
    "    # 数值字段\n",
    "    numeric_features = [ conver_float(column)  for column in fields[4: end_index]]\n",
    "    # print numeric_features\n",
    "    \n",
    "    # 返回  类别 特征 +  数值特征 \n",
    "    return np.concatenate((category_features, numeric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 标签字段\n",
    "def extract_label(fields):\n",
    "    label = fields[-1]\n",
    "    return float(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.89131000e-01, 2.05555556e+00,\n",
       "       6.76470588e-01, 2.05882353e-01, 4.70588240e-02, 2.35294120e-02,\n",
       "       4.43783175e-01, 0.00000000e+00, 0.00000000e+00, 9.07738100e-02,\n",
       "       0.00000000e+00, 2.45831182e-01, 3.88349500e-03, 1.00000000e+00,\n",
       "       1.00000000e+00, 2.40000000e+01, 0.00000000e+00, 5.42400000e+03,\n",
       "       1.70000000e+02, 8.00000000e+00, 1.52941176e-01, 7.91295750e-02])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试 提取特征函数\n",
    "sample_data = datas.first()\n",
    "extract_features(sample_data, catetory_dic, len(sample_data)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LabeledPoint in module pyspark.mllib.regression:\n",
      "\n",
      "class LabeledPoint(builtins.object)\n",
      " |  Class that represents the features and labels of a data point.\n",
      " |  \n",
      " |  :param label:\n",
      " |    Label for this data point.\n",
      " |  :param features:\n",
      " |    Vector of features for this point (NumPy array, list,\n",
      " |    pyspark.mllib.linalg.SparseVector, or scipy.sparse column matrix).\n",
      " |  \n",
      " |  .. note:: 'label' and 'features' are accessible as class attributes.\n",
      " |  \n",
      " |  .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, label, features)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "help(LabeledPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程：构建分类算法特征RDD： LabeledPoint\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "labelpoint_rdd = datas.map(lambda r: \n",
    "          LabeledPoint(extract_label(r), extract_features(r, catetory_dic, len(r)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', '0.789131', '2.055555556', '0.676470588', '0.205882353', '0.047058824', '0.023529412', '0.443783175', '0', '0', '0.09077381', '0', '0.245831182', '0.003883495', '1', '1', '24', '0', '5424', '170', '8', '0.152941176', '0.079129575', '0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据对比\n",
    "\n",
    "print(datas.first()[3:])\n",
    "\n",
    "labelpoint_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据划分（训练集、验证集和测试集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method randomSplit in module pyspark.rdd:\n",
      "\n",
      "randomSplit(weights, seed=None) method of pyspark.rdd.PipelinedRDD instance\n",
      "    Randomly splits this RDD with the provided weights.\n",
      "    \n",
      "    :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      "    :param seed: random seed\n",
      "    :return: split RDDs in a list\n",
      "    \n",
      "    >>> rdd = sc.parallelize(range(500), 1)\n",
      "    >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      "    >>> len(rdd1.collect() + rdd2.collect())\n",
      "    500\n",
      "    >>> 150 < rdd1.count() < 250\n",
      "    True\n",
      "    >>> 250 < rdd2.count() < 350\n",
      "    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(labelpoint_rdd.randomSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集，按照 8: 1: 1划分\n",
    "train_rdd, validation_rdd, test_rdd = labelpoint_rdd.randomSplit([8,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集:  5906\n",
      "验证数据集:  701\n",
      "测试数据集:  788\n"
     ]
    }
   ],
   "source": [
    "# 统计各个数据集条目数，并且将数据缓存起来\n",
    "print('训练数据集: ', train_rdd.cache().count())\n",
    "print ('验证数据集: ', validation_rdd.cache().count())\n",
    "print ('测试数据集: ', test_rdd.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTree in module pyspark.mllib.tree:\n",
      "\n",
      "class DecisionTree(builtins.object)\n",
      " |  Learning algorithm for a decision tree model for classification or\n",
      " |  regression.\n",
      " |  \n",
      " |  .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  trainClassifier(data, numClasses, categoricalFeaturesInfo, impurity='gini', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) from builtins.type\n",
      " |      Train a decision tree model for classification.\n",
      " |      \n",
      " |      :param data:\n",
      " |        Training data: RDD of LabeledPoint. Labels should take values\n",
      " |        {0, 1, ..., numClasses-1}.\n",
      " |      :param numClasses:\n",
      " |        Number of classes for classification.\n",
      " |      :param categoricalFeaturesInfo:\n",
      " |        Map storing arity of categorical features. An entry (n -> k)\n",
      " |        indicates that feature n is categorical with k categories\n",
      " |        indexed from 0: {0, 1, ..., k-1}.\n",
      " |      :param impurity:\n",
      " |        Criterion used for information gain calculation.\n",
      " |        Supported values: \"gini\" or \"entropy\".\n",
      " |        (default: \"gini\")\n",
      " |      :param maxDepth:\n",
      " |        Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n",
      " |        means 1 internal node + 2 leaf nodes).\n",
      " |        (default: 5)\n",
      " |      :param maxBins:\n",
      " |        Number of bins used for finding splits at each node.\n",
      " |        (default: 32)\n",
      " |      :param minInstancesPerNode:\n",
      " |        Minimum number of instances required at child nodes to create\n",
      " |        the parent split.\n",
      " |        (default: 1)\n",
      " |      :param minInfoGain:\n",
      " |        Minimum info gain required to create a split.\n",
      " |        (default: 0.0)\n",
      " |      :return:\n",
      " |        DecisionTreeModel.\n",
      " |      \n",
      " |      Example usage:\n",
      " |      \n",
      " |      >>> from numpy import array\n",
      " |      >>> from pyspark.mllib.regression import LabeledPoint\n",
      " |      >>> from pyspark.mllib.tree import DecisionTree\n",
      " |      >>>\n",
      " |      >>> data = [\n",
      " |      ...     LabeledPoint(0.0, [0.0]),\n",
      " |      ...     LabeledPoint(1.0, [1.0]),\n",
      " |      ...     LabeledPoint(1.0, [2.0]),\n",
      " |      ...     LabeledPoint(1.0, [3.0])\n",
      " |      ... ]\n",
      " |      >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})\n",
      " |      >>> print(model)\n",
      " |      DecisionTreeModel classifier of depth 1 with 3 nodes\n",
      " |      \n",
      " |      >>> print(model.toDebugString())\n",
      " |      DecisionTreeModel classifier of depth 1 with 3 nodes\n",
      " |        If (feature 0 <= 0.0)\n",
      " |         Predict: 0.0\n",
      " |        Else (feature 0 > 0.0)\n",
      " |         Predict: 1.0\n",
      " |      <BLANKLINE>\n",
      " |      >>> model.predict(array([1.0]))\n",
      " |      1.0\n",
      " |      >>> model.predict(array([0.0]))\n",
      " |      0.0\n",
      " |      >>> rdd = sc.parallelize([[1.0], [0.0]])\n",
      " |      >>> model.predict(rdd).collect()\n",
      " |      [1.0, 0.0]\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  trainRegressor(data, categoricalFeaturesInfo, impurity='variance', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) from builtins.type\n",
      " |      Train a decision tree model for regression.\n",
      " |      \n",
      " |      :param data:\n",
      " |        Training data: RDD of LabeledPoint. Labels are real numbers.\n",
      " |      :param categoricalFeaturesInfo:\n",
      " |        Map storing arity of categorical features. An entry (n -> k)\n",
      " |        indicates that feature n is categorical with k categories\n",
      " |        indexed from 0: {0, 1, ..., k-1}.\n",
      " |      :param impurity:\n",
      " |        Criterion used for information gain calculation.\n",
      " |        The only supported value for regression is \"variance\".\n",
      " |        (default: \"variance\")\n",
      " |      :param maxDepth:\n",
      " |        Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n",
      " |        means 1 internal node + 2 leaf nodes).\n",
      " |        (default: 5)\n",
      " |      :param maxBins:\n",
      " |        Number of bins used for finding splits at each node.\n",
      " |        (default: 32)\n",
      " |      :param minInstancesPerNode:\n",
      " |        Minimum number of instances required at child nodes to create\n",
      " |        the parent split.\n",
      " |        (default: 1)\n",
      " |      :param minInfoGain:\n",
      " |        Minimum info gain required to create a split.\n",
      " |        (default: 0.0)\n",
      " |      :return:\n",
      " |        DecisionTreeModel.\n",
      " |      \n",
      " |      Example usage:\n",
      " |      \n",
      " |      >>> from pyspark.mllib.regression import LabeledPoint\n",
      " |      >>> from pyspark.mllib.tree import DecisionTree\n",
      " |      >>> from pyspark.mllib.linalg import SparseVector\n",
      " |      >>>\n",
      " |      >>> sparse_data = [\n",
      " |      ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      " |      ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n",
      " |      ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      " |      ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n",
      " |      ... ]\n",
      " |      >>>\n",
      " |      >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n",
      " |      >>> model.predict(SparseVector(2, {1: 1.0}))\n",
      " |      1.0\n",
      " |      >>> model.predict(SparseVector(2, {1: 0.0}))\n",
      " |      0.0\n",
      " |      >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\n",
      " |      >>> model.predict(rdd).collect()\n",
      " |      [1.0, 0.0]\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入模块\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "help(DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeModel classifier of depth 5 with 59 nodes"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "# trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, \n",
    "#    impurity='gini', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) \n",
    "dt_model = DecisionTree.trainClassifier(train_rdd, 2, {}, \n",
    "                                        impurity='entropy', maxDepth=5, maxBins=32)\n",
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 5 with 59 nodes\n",
      "  If (feature 31 <= 1549.0)\n",
      "   If (feature 3 <= 0.0)\n",
      "    If (feature 12 <= 0.0)\n",
      "     If (feature 31 <= 1074.0)\n",
      "      If (feature 23 <= 0.03930131)\n",
      "       Predict: 0.0\n",
      "      Else (feature 23 > 0.03930131)\n",
      "       Predict: 0.0\n",
      "     Else (feature 31 > 1074.0)\n",
      "      If (feature 11 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 11 > 0.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 12 > 0.0)\n",
      "     If (feature 23 <= 0.124260355)\n",
      "      If (feature 14 <= 0.858425)\n",
      "       Predict: 1.0\n",
      "      Else (feature 14 > 0.858425)\n",
      "       Predict: 1.0\n",
      "     Else (feature 23 > 0.124260355)\n",
      "      If (feature 14 <= 0.826791)\n",
      "       Predict: 0.0\n",
      "      Else (feature 14 > 0.826791)\n",
      "       Predict: 1.0\n",
      "   Else (feature 3 > 0.0)\n",
      "    If (feature 35 <= 0.102079395)\n",
      "     If (feature 35 <= 0.086696562)\n",
      "      If (feature 23 <= 0.077868852)\n",
      "       Predict: 0.0\n",
      "      Else (feature 23 > 0.077868852)\n",
      "       Predict: 0.0\n",
      "     Else (feature 35 > 0.086696562)\n",
      "      Predict: 0.0\n",
      "    Else (feature 35 > 0.102079395)\n",
      "     If (feature 29 <= 67.0)\n",
      "      If (feature 34 <= 0.063197026)\n",
      "       Predict: 0.0\n",
      "      Else (feature 34 > 0.063197026)\n",
      "       Predict: 0.0\n",
      "     Else (feature 29 > 67.0)\n",
      "      If (feature 17 <= 0.075)\n",
      "       Predict: 0.0\n",
      "      Else (feature 17 > 0.075)\n",
      "       Predict: 1.0\n",
      "  Else (feature 31 > 1549.0)\n",
      "   If (feature 1 <= 0.0)\n",
      "    If (feature 11 <= 0.0)\n",
      "     If (feature 0 <= 0.0)\n",
      "      If (feature 29 <= 25.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 29 > 25.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 0.0)\n",
      "      If (feature 26 <= 0.031100478)\n",
      "       Predict: 1.0\n",
      "      Else (feature 26 > 0.031100478)\n",
      "       Predict: 1.0\n",
      "    Else (feature 11 > 0.0)\n",
      "     If (feature 23 <= 0.037135279)\n",
      "      If (feature 26 <= 0.005754759)\n",
      "       Predict: 0.0\n",
      "      Else (feature 26 > 0.005754759)\n",
      "       Predict: 1.0\n",
      "     Else (feature 23 > 0.037135279)\n",
      "      If (feature 15 <= 2.538461538)\n",
      "       Predict: 1.0\n",
      "      Else (feature 15 > 2.538461538)\n",
      "       Predict: 1.0\n",
      "   Else (feature 1 > 0.0)\n",
      "    If (feature 29 <= 30.0)\n",
      "     If (feature 26 <= 0.01189387)\n",
      "      If (feature 20 <= 0.459308807)\n",
      "       Predict: 1.0\n",
      "      Else (feature 20 > 0.459308807)\n",
      "       Predict: 0.0\n",
      "     Else (feature 26 > 0.01189387)\n",
      "      If (feature 14 <= 0.46825)\n",
      "       Predict: 0.0\n",
      "      Else (feature 14 > 0.46825)\n",
      "       Predict: 0.0\n",
      "    Else (feature 29 > 30.0)\n",
      "     If (feature 15 <= 2.137640449)\n",
      "      If (feature 35 <= 0.051282051)\n",
      "       Predict: 1.0\n",
      "      Else (feature 35 > 0.051282051)\n",
      "       Predict: 0.0\n",
      "     Else (feature 15 > 2.137640449)\n",
      "      Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dt_model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于训练的模型和验证书籍及 评估\n",
    "score = dt_model.predict(validation_rdd.map(lambda lp: lp.features))\n",
    "\n",
    "# 组合预测的值和实际真实的值\n",
    "score_and_label = score.zip(validation_rdd.map(lambda lp: lp.label))\n",
    "\n",
    "# 获取前十个数据打印\n",
    "score_and_label.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    使用AUC（Area under the Curve of ROC）评估二分类模型，\n",
    "    -a. AUC = 1\n",
    "        最完美的情况，预测准确率到100%，但是不可能存在\n",
    "    -b. 0.5  < AUC < 1\n",
    "        优于随机猜测，具有预测的价值\n",
    "    -c. AUC = 0.5\n",
    "        余随机猜测一样，没有任何预测价值\n",
    "    -d. AUC < 0.5\n",
    "        适合于反向预测\n",
    "        \n",
    "    ROC曲线  PR曲线： 精确度precision 和 召回率 之间的关系\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.6440093479252166\n",
      "PR:  0.7437037604923291\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# 使用BinaryClassificationMetrics 计算AUC面积\n",
    "dt_metrics = BinaryClassificationMetrics(score_and_label)\n",
    "\n",
    "print(\"AUC: \", dt_metrics.areaUnderROC)\n",
    "print(\"PR: \", dt_metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# 方便对模型评估，定义函数\n",
    "def evaluate_mode(model, validation_data):\n",
    "    # 基于训练的模型和验证书籍及 评估\n",
    "    score = model.predict(validation_data.map(lambda lp: lp.features))\n",
    "\n",
    "    # 组合预测的值和实际真实的值\n",
    "    score_and_label = score.zip(validation_data.map(lambda lp: lp.label))\n",
    "    \n",
    "    # 使用BinaryClassificationMetrics 计算AUC面积\n",
    "    metrics = BinaryClassificationMetrics(score_and_label)\n",
    "\n",
    "    return metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    构建一个函数，实现模型的 训练和评估功能，并且计算评估所需的时间\n",
    "\"\"\"\n",
    "from time import time\n",
    "\n",
    "def train_evaluate_model(train_data, validation_data, param_impurity, param_depth, param_bins):\n",
    "    # 模型训练开始时间\n",
    "    start_time = time()\n",
    "    \n",
    "    # 训练模型\n",
    "    model = DecisionTree.trainClassifier(train_rdd, 2, {}, \n",
    "           impurity=param_impurity, maxDepth=param_depth, maxBins=param_bins)\n",
    "    \n",
    "    # 模型验证评估\n",
    "    auc = evaluate_mode(model, validation_data)\n",
    "    \n",
    "    # 计算花费时间\n",
    "    duration = time() - start_time\n",
    "    print(\"训练评估使用参数：impurity = \" + str(param_impurity) + \\\n",
    "        \", maxDepth = \" + str(param_depth) + \\\n",
    "        \", maxBins = \" + str(param_bins) + \\\n",
    "        \" => 所需时间 = \" + str(duration) + \", 评估AUC = \" + str(auc))\n",
    "    \n",
    "    # 返回\n",
    "    return (auc, duration, param_impurity, param_depth, param_bins, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练评估使用参数：impurity = entropy, maxDepth = 5, maxBins = 32 => 所需时间 = 6.835391044616699, 评估AUC = 0.6440093479252166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6440093479252166,\n",
       " 6.835391044616699,\n",
       " 'entropy',\n",
       " 5,\n",
       " 32,\n",
       " DecisionTreeModel classifier of depth 5 with 59 nodes)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试函数\n",
    "train_evaluate_model(train_rdd, validation_rdd, \"entropy\", 5, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练评估使用参数：impurity = entropy, maxDepth = 5, maxBins = 16 => 所需时间 = 7.342419862747192, 评估AUC = 0.634205426356589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.634205426356589,\n",
       " 7.342419862747192,\n",
       " 'entropy',\n",
       " 5,\n",
       " 16,\n",
       " DecisionTreeModel classifier of depth 5 with 61 nodes)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evaluate_model(train_rdd, validation_rdd, \"entropy\", 5, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评估不同impurity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练评估使用参数：impurity = entropy, maxDepth = 10, maxBins = 8 => 所需时间 = 7.216412544250488, 评估AUC = 0.6370594749527717\n",
      "训练评估使用参数：impurity = gini, maxDepth = 10, maxBins = 8 => 所需时间 = 7.559432506561279, 评估AUC = 0.6289736824962543\n"
     ]
    }
   ],
   "source": [
    "impurity_list = ['entropy', 'gini']\n",
    "depth_list = [10]\n",
    "bins_list = [8]\n",
    "\n",
    "metrics_list = [ train_evaluate_model(train_rdd, validation_rdd, impurity, depth, bins)\n",
    "    for impurity in impurity_list\n",
    "    for depth in depth_list\n",
    "    for bins in bins_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6370594749527717,\n",
       "  7.216412544250488,\n",
       "  'entropy',\n",
       "  10,\n",
       "  8,\n",
       "  DecisionTreeModel classifier of depth 10 with 561 nodes),\n",
       " (0.6289736824962543,\n",
       "  7.559432506561279,\n",
       "  'gini',\n",
       "  10,\n",
       "  8,\n",
       "  DecisionTreeModel classifier of depth 10 with 697 nodes)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写函数，可以对决策树 任意的参数调整进行训练和评估\n",
    "def train_evaluate_parameter(train_datas, validation_datas, \n",
    "                            impurity_list, depth_list, bins_list):\n",
    "    # 训练及评估返回值\n",
    "    metrics_list = [ train_evaluate_model(train_rdd, validation_rdd, impurity, depth, bins)\n",
    "                        for impurity in impurity_list\n",
    "                        for depth in depth_list\n",
    "                        for bins in bins_list\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用不同maxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_parameter(train_rdd, validation_rdd, ['entropy'], [5, 10, 15, 20, 25], [8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用不同的maxBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_parameter(train_rdd, validation_rdd, ['entropy'], [10], [4, 8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同超参数组合，训练与评估，找到最佳模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数\n",
    "# 编写函数，可以对决策树 任意的参数调整进行训练和评估\n",
    "def train_evaluate_params(train_datas, validation_datas, \n",
    "                            impurity_list, depth_list, bins_list):\n",
    "    # 训练及评估返回值\n",
    "    metrics_list = [ train_evaluate_model(train_rdd, validation_rdd, impurity, depth, bins)\n",
    "                        for impurity in impurity_list\n",
    "                        for depth in depth_list\n",
    "                        for bins in bins_list\n",
    "                    ]\n",
    "    \n",
    "    # 针对 auc值降序排序，找出最佳 模型\n",
    "    sorted_metrics_list = sorted(metrics_list, key=lambda k: k[0], reverse=True)\n",
    "    \n",
    "    # 获取最佳模型\n",
    "    best_params = sorted_metrics_list[0]\n",
    "    \n",
    "    # 打印显示，最佳参数组合\n",
    "    print(\"最佳参数组合: impurity -> \" + str(best_params[2]) + \\\n",
    "             \", depth -> \" + str(best_params[3]) + \\\n",
    "              \", bins -> \" + str(best_params[4]) + \\\n",
    "              \"\\n AUC -> \" + str(best_params[0])\n",
    "         )\n",
    "    \n",
    "    # 返回模型\n",
    "    return best_params[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "print ('-------------------- 设置不同超参数的不同值进行训练评估 -------------------')\n",
    "best_model = train_evaluate_params(train_rdd, validation_rdd,\n",
    "              ['gini', 'entropy'], [5, 10, 15, 20, 25], [4, 8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 使用 测试数据集 ，最佳模型是否过拟合\n",
    "auc_metric = evaluate_mode(best_model, test_rdd)\n",
    "\n",
    "print '基于测试数据集评估AUC = ', auc_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将最佳模型进行保存\n",
    "best_model.save(sc, path='./datas/dtc-best-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTreeModel \n",
    "\n",
    "load_dtc_model = DecisionTreeModel.load(sc, path='./datas/dtc-best-model')\n",
    "\n",
    "load_dtc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(load_dtc_model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他分类算法 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "help(SVMWithSGD)\n",
    "\n",
    "\"\"\"\n",
    "     iterations=100, step=1.0, miniBatchFraction=1.0\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "help(LogisticRegressionWithLBFGS)\n",
    "\n",
    "\"\"\"\n",
    "    iterations=100， regType='l2'\n",
    "\"\"\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "help(NaiveBayes)\n",
    "\n",
    "\"\"\"\n",
    "    lambda_=1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
